{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "lab_tagging_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qNj1-ogKNVzJ"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2L5rsEeOwA3D",
        "colab_type": "text"
      },
      "source": [
        "# Практическое задание 2 (часть 2)\n",
        "\n",
        "# Распознавание именованных сущностей из Twitter с помощью LSTM\n",
        "\n",
        "## курс \"Математические методы анализа текстов\"\n",
        "\n",
        "\n",
        "### ФИО: Хуршудов Артем Эрнестович"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIIo2ChwA3F",
        "colab_type": "text"
      },
      "source": [
        "## Введение\n",
        "\n",
        "### Постановка задачи\n",
        "\n",
        "В этом задании вы будете использовать рекуррентные нейронные сети для решения проблемы распознавания именованных сущностей (NER). Примерами именованных сущностей являются имена людей, названия организаций, адреса и т.д. В этом задании вы будете работать с данными twitter.\n",
        "\n",
        "Например, вы хотите извлечь имена и названия организаций. Тогда для текста\n",
        "\n",
        "    Yan Goodfellow works for Google Brain\n",
        "\n",
        "модель должна извлечь следующую последовательность:\n",
        "\n",
        "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
        "\n",
        "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система введена, чтобы различать последовательные именованные сущности одного типа.\n",
        "\n",
        "Решение этого задания будет основано на нейронных сетях, а именно на Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
        "\n",
        "### Библиотеки\n",
        "\n",
        "Для этого задания вам понадобятся следующие библиотеки:\n",
        " - [Pytorch](https://pytorch.org/).\n",
        " - [Numpy](http://www.numpy.org).\n",
        " \n",
        "### Данные\n",
        "\n",
        "Все данные содержатся в папке `./data`: `./data/train.txt`, `./data/validation.txt`, `./data/test.txt`.\n",
        "\n",
        "Скачать архив можно здесь: [ссылка на google диск](https://drive.google.com/open?id=1s1rFOFMZTBqtJuQDcIvW-8djA78iUDcx)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTjA4OjEwA3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wAu-o9NN4Lu",
        "colab_type": "code",
        "outputId": "5318f519-eb21-4250-efbe-1727b1790eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxoTfYuOwA3L",
        "colab_type": "text"
      },
      "source": [
        "## Часть 1. Подготовка данных (2 балла)\n",
        "\n",
        "### Загрузка данных\n",
        "\n",
        "Мы будем работать с данными, которые содержат твиты с тегами именованных сущностей. Каждая строка файла содержит пару токен(слово или пунктуация) и тег, разделенные пробелом. Различные твиты разделены пустой строкой.\n",
        "\n",
        "Функция *read_data* считывает корпус из *file_path* и возвращает два списка: один с токенами и один с соответствующими токенам тегами. Также она заменяет все ники (токены, которые начинаются на символ *@*) на токен `<USR>` и url-ы (токены, которые начинаются на *http://* или *https://*) на токен `<URL>`. \n",
        "\n",
        "Вам необходимо реализовать эту функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xxCuz__wA3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    arr = []\n",
        "    tmp = []\n",
        "    for el in lines:\n",
        "      if el != '\\n':\n",
        "        tmp.append(el)\n",
        "      else:\n",
        "        arr.append(tmp)\n",
        "        tmp = []\n",
        "    tokens = [[el.split()[0] for el in sent] \n",
        "              for sent in arr]\n",
        "    tags = [[el.split()[1] for el in sent] \n",
        "              for sent in arr]\n",
        "\n",
        "    for sent_idx in range(len(tokens)):\n",
        "      for token_idx in range(len(tokens[sent_idx])):\n",
        "        if tokens[sent_idx][token_idx][0] == '@':\n",
        "          tags[sent_idx][token_idx] = '<USR>'\n",
        "        if (tokens[sent_idx][token_idx][:7] == 'http://') or \\\n",
        "           (tokens[sent_idx][token_idx][:8] == 'https://'):\n",
        "          tags[sent_idx][token_idx] = '<URL>'\n",
        "    \n",
        "    return tokens, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvG6hYt8wA3P",
        "colab_type": "text"
      },
      "source": [
        "Теперь мы можем загрузить 3 части данных:\n",
        " - *train* для тренировки модели;\n",
        " - *validation* для валидации и подбора гиперпараметров;\n",
        " - *test* для финального тестирования."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxRryDGYwA3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_dir = 'gdrive/My Drive/mmta/RNN/'\n",
        "\n",
        "train_tokens, train_tags = read_data(_dir + 'data/train.txt')\n",
        "validation_tokens, validation_tags = read_data(_dir + 'data/validation.txt')\n",
        "test_tokens, test_tags = read_data(_dir + 'data/test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiAoUwKWwA3U",
        "colab_type": "text"
      },
      "source": [
        "Всегда полезно знать, с какими данными вы работаете. Выведем небольшую часть."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPEsU3oKwA3V",
        "colab_type": "code",
        "outputId": "8710bd01-a6f8-4f27-b809-d780bf0bb784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RT\tO\n",
            "@TheValarium\t<USR>\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "http://t.co/2zgQ99nmuf\t<URL>\n",
            "http://t.co/eQmogqqABK\t<URL>\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "@AshForeverAshey\t<USR>\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifY8R4vtwA3X",
        "colab_type": "text"
      },
      "source": [
        "### Подготовка словарей\n",
        "\n",
        "Чтобы обучать нейронную сеть, мы будем использовать два отображения.\n",
        "\n",
        "- {token}$\\to${token id}: устанавливает соответствие между токеном и строкой в embedding матрице;\n",
        "- {tag}$\\to${tag id}: one hot encoding тегов.\n",
        "\n",
        "\n",
        "Теперь вам необходимо реализовать функцию *build_dict*, которая должна возвращать словарь {token or tag}$\\to${index} и контейнер, задающий обратное отображение."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIYraADPwA3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf5FVqZhwA3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "    tokens_or_tags: a list of lists of tokens or tags\n",
        "    special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens to indices and vice versa\n",
        "    # Add special tokens to dictionaries\n",
        "    # The first special token must have index 0\n",
        "\n",
        "    for idx, el in enumerate(special_tokens):\n",
        "      tok2idx[el] = idx\n",
        "      idx2tok.append(el)\n",
        "      \n",
        "    all_tokens = set([el \n",
        "                  for sent in tokens_or_tags\n",
        "                  for el in sent])\n",
        "    \n",
        "    for el in special_tokens:\n",
        "      if el in all_tokens:\n",
        "        all_tokens.remove(el)\n",
        "    \n",
        "    for idx, el in enumerate(all_tokens):\n",
        "      tok2idx[el] = idx + len(special_tokens)\n",
        "      idx2tok.append(el)\n",
        "    \n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtOqVL6kwA3d",
        "colab_type": "text"
      },
      "source": [
        "После реализации функции *build_dict* вы можете создать словари для токенов и тегов. В нашем случае специальными токенами будут:\n",
        " - `<UNK>` токен для обозначаения слов, которых нет в словаре;\n",
        " - `<PAD>` токен для дополнения предложений одного батча до одинаковой длины."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIPjxBJEwA3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK_fmvYqwA3h",
        "colab_type": "text"
      },
      "source": [
        "### Генератор батчей\n",
        "\n",
        "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины. Для некоторых фреймворков (таких как tensorflow) это необходимо сделать до подачи батча в нейронную сеть. В случае с pytorch это можно сделать как вне архитектуры нейронной сети, так и внутри. Мы выбрали более универсальный вариант и наш генератор батчей дополняет все последовательности внутри одного батча до одной длины.\n",
        "\n",
        "Генератор батчей разбивает последовательность входных предложений и тегов на батчи размера batch_size. Размер последнего батча может быть меньше, если allow_smaller_last_batch is True, иначе последний батч исключается из генератора. Если включён параметр shuffle, данные перед разделением на батчи будут перемешаны. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1rlcfNJwA3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def batches_generator(batch_size, tokens_idxs, tags_idxs,\n",
        "                      shuffle=True, allow_smaller_last_batch=True, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generates padded batches of tags_idxs and tags_idxs.\n",
        "    \n",
        "    batch_size : int, number of objects in one batch\n",
        "    tokens_idxs : list of list of int\n",
        "    tags_idxs : list of list of int\n",
        "    shuffle : bool\n",
        "    allow_smaller_last_batch : bool\n",
        "    device: str, cpu or cuda:x\n",
        "    \n",
        "    yield x, y: torch.LongTensor and torch.LongTensor\n",
        "    x - batch of tokens_idxs, y - batch of tags_idxs\n",
        "    \"\"\"\n",
        "    n_samples = len(tokens_idxs)\n",
        "    \n",
        "    tokens_idxs = np.array(tokens_idxs)\n",
        "    tags_idxs = np.array(tags_idxs)\n",
        "    \n",
        "    if shuffle:\n",
        "      idxs = list(range(len(tokens_idxs)))\n",
        "      random.shuffle(idxs)\n",
        "      tokens_idxs = tokens_idxs[idxs]\n",
        "      tags_idxs = tags_idxs[idxs]\n",
        "    \n",
        "    # Get the number of batches\n",
        "    n_batches = int(n_samples / batch_size)\n",
        "    if n_samples%batch_size: \n",
        "      n_batches += allow_smaller_last_batch\n",
        "      \n",
        "    \n",
        "    # For each k yield pair x and y\n",
        "    for k in range(n_batches):\n",
        "        cur_sent = tokens_idxs[k*batch_size : (k+1)*batch_size]\n",
        "        cur_tags = tags_idxs[k*batch_size : (k+1)*batch_size]\n",
        "        sent_len = max([len(el) for el in cur_sent])\n",
        "\n",
        "        for i in range(len(cur_sent)):\n",
        "          while len(cur_sent[i]) < sent_len:\n",
        "            cur_sent[i].append(token2idx['<PAD>'])\n",
        "            cur_tags[i].append(tag2idx['O'])\n",
        "        \n",
        "        cur_sent = np.array([np.array(el) for el in cur_sent])\n",
        "        cur_tags = np.array([np.array(el) for el in cur_tags])\n",
        "        \n",
        "        \n",
        "        x = torch.LongTensor(cur_sent)\n",
        "        y = torch.LongTensor(cur_tags)\n",
        "        \n",
        "        x = x.long().to(device)\n",
        "        y = y.long().to(device)\n",
        "        \n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24e8bZ_nwA3n",
        "colab_type": "text"
      },
      "source": [
        "Протестируйте ваш генератор батчей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATBeDqCynPEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_idxs = [[token2idx[el] for el in sent] for sent in train_tokens]\n",
        "train_tag_idxs = [[tag2idx[el] for el in sent] for sent in train_tags]\n",
        "\n",
        "test_idxs = [[token2idx[el] for el in sent] for sent in test_tokens]\n",
        "test_tag_idxs = [[tag2idx[el] for el in sent] for sent in test_tags]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rvY6wIYKwCf",
        "colab_type": "code",
        "outputId": "de218c18-7956-486e-e840-510e2cba3136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "np.array(train_idxs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([2934, 26243, 16661, 1146, 19044, 7027, 23503, 3304, 13647, 19264, 22757, 10796, 20368, 4271, 8397, 23272, 14165, 25910, 20958, 24657, 17738, 23675, 12604, 489, 9173, 22350]),\n",
              "       list([16245, 5719, 19357, 10098, 21602, 5217, 22662, 4105, 9710, 25407, 13794, 9746, 5355, 1052, 4105, 14743, 11474, 15191, 10722, 25714, 13434]),\n",
              "       list([19841, 24276, 19040, 2413, 17909, 14321, 10237, 17920, 424, 17712, 21366, 8247, 21368, 20958]),\n",
              "       ...,\n",
              "       list([23248, 24580, 4018, 20825, 5631, 15603, 4478, 9314, 24785, 19167, 10521, 16030, 3554, 920, 13150, 15248, 18606, 10086, 15638, 19980]),\n",
              "       list([9551, 10183, 22462, 15034, 12048, 20739, 15993, 7666, 4696, 19591, 13467, 9604, 23765]),\n",
              "       list([14016, 16625, 15688, 26704, 18180, 526, 22272, 17418, 21107, 20958])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn60UzQrwA3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "test_nonrandom_batch_generator = batches_generator(\n",
        "    batch_size=3,\n",
        "    tokens_idxs=train_idxs[:7],\n",
        "    tags_idxs=train_tag_idxs[:7],\n",
        "    shuffle=False,\n",
        "    allow_smaller_last_batch=True\n",
        ")\n",
        "\n",
        "batch_lengths = [3, 3, 1]\n",
        "sequence_lengths = [26, 25, 8]\n",
        "some_pad_tensor = torch.LongTensor([token2idx['<PAD>']] * 12)\n",
        "some_outside_tensor = torch.LongTensor([tag2idx['O'] * 12])\n",
        "\n",
        "for i, (tokens_batch, tags_batch) in enumerate(test_nonrandom_batch_generator):\n",
        "    assert tokens_batch.dtype == torch.int64, 'tokens_batch is not LongTensor'\n",
        "    assert tags_batch.dtype == torch.int64, 'tags_batch is not LongTensor'\n",
        "    \n",
        "    assert len(tokens_batch) == batch_lengths[i], 'wrong batch length'\n",
        "    \n",
        "    for one_token_sequence in tokens_batch:\n",
        "        assert len(one_token_sequence) == sequence_lengths[i], 'wrong length of sequence in batch'\n",
        "    \n",
        "    if i == 0:\n",
        "        assert torch.all(tokens_batch[2][-12:] == some_pad_tensor), \"wrong padding\"       \n",
        "        assert torch.all(tags_batch[2][-12:] == some_outside_tensor), \"wrong O tag\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP0SrulJwA3q",
        "colab_type": "text"
      },
      "source": [
        "## Часть 2. BiLSTM (3 балла)\n",
        "\n",
        "Определите архитектуру сети, используя библиотеку pytorch. \n",
        "\n",
        "**Замечания:**\n",
        "1. Для улучшения качества сети предлагается использовать дополнительный Embedding слой на входе (каждому слову ставится в соответствие обучаемый вектор). \n",
        "\n",
        "2. Не забудьте, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNj1-ogKNVzJ",
        "colab_type": "text"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QdjBAkowA3r",
        "colab_type": "text"
      },
      "source": [
        "Для тестирования сети мы подготовили для вас две функции:\n",
        " - *predict_tags*: получает батч данных и трансформирует его в список из токенов и предсказанных тегов;\n",
        " - *eval_conll*: вычисляет метрики precision, recall и F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shkKpX0kwA3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from evaluation_ner import precision_recall_f1\n",
        "from collections import OrderedDict\n",
        "\n",
        "def _update_chunk(candidate, prev, current_tag, current_chunk, current_pos, prediction=False):\n",
        "    if candidate == 'B-' + current_tag:\n",
        "        if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
        "                current_chunk[-1].append(current_pos - 1)\n",
        "        current_chunk.append([current_pos])\n",
        "    elif candidate == 'I-' + current_tag:\n",
        "        if prediction and (current_pos == 0 or current_pos > 0 and prev.split('-', 1)[-1] != current_tag):\n",
        "            current_chunk.append([current_pos])\n",
        "        if not prediction and (current_pos == 0 or current_pos > 0 and prev == 'O'):\n",
        "            current_chunk.append([current_pos])\n",
        "    elif current_pos > 0 and prev.split('-', 1)[-1] == current_tag:\n",
        "        if len(current_chunk) > 0:\n",
        "            current_chunk[-1].append(current_pos - 1)\n",
        "\n",
        "def _update_last_chunk(current_chunk, current_pos):\n",
        "    if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
        "        current_chunk[-1].append(current_pos - 1)\n",
        "\n",
        "def _tag_precision_recall_f1(tp, fp, fn):\n",
        "    precision, recall, f1 = 0, 0, 0\n",
        "    if tp + fp > 0:\n",
        "        precision = tp / (tp + fp) * 100\n",
        "    if tp + fn > 0:\n",
        "        recall = tp / (tp + fn) * 100\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    return precision, recall, f1\n",
        "\n",
        "def _aggregate_metrics(results, total_correct):\n",
        "    total_true_entities = 0\n",
        "    total_predicted_entities = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1 = 0\n",
        "    for tag, tag_metrics in results.items():\n",
        "        n_pred = tag_metrics['n_predicted_entities']\n",
        "        n_true = tag_metrics['n_true_entities']\n",
        "        total_true_entities += n_true\n",
        "        total_predicted_entities += n_pred\n",
        "        total_precision += tag_metrics['precision'] * n_pred\n",
        "        total_recall += tag_metrics['recall'] * n_true\n",
        "    accuracy = total_correct / total_true_entities * 100\n",
        "    total_precision = total_precision / total_predicted_entities if total_predicted_entities != 0 else 0\n",
        "    total_recall = total_recall / total_true_entities\n",
        "    if total_precision + total_recall > 0:\n",
        "        if total_precision + total_recall >= 1e-16:\n",
        "            total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n",
        "        else:\n",
        "            total_f1 = 0\n",
        "    return total_true_entities, total_predicted_entities, \\\n",
        "           total_precision, total_recall, total_f1, accuracy\n",
        "\n",
        "def _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct):\n",
        "    print('processed {len} tokens ' \\\n",
        "          'with {tot_true} phrases; ' \\\n",
        "          'found: {tot_pred} phrases; ' \\\n",
        "          'correct: {tot_cor}.\\n'.format(len=n_tokens,\n",
        "                                         tot_true=total_true_entities,\n",
        "                                         tot_pred=total_predicted_entities,\n",
        "                                         tot_cor=total_correct))\n",
        "\n",
        "def _print_metrics(accuracy, total_precision, total_recall, total_f1):\n",
        "    print('precision:  {tot_prec:.2f}%; ' \\\n",
        "          'recall:  {tot_recall:.2f}%; ' \\\n",
        "          'F1:  {tot_f1:.2f}\\n'.format(acc=accuracy,\n",
        "                                           tot_prec=total_precision,\n",
        "                                           tot_recall=total_recall,\n",
        "                                           tot_f1=total_f1))\n",
        "\n",
        "def _print_tag_metrics(tag, tag_results):\n",
        "    print(('\\t%12s' % tag) + ': precision:  {tot_prec:6.2f}%; ' \\\n",
        "                               'recall:  {tot_recall:6.2f}%; ' \\\n",
        "                               'F1:  {tot_f1:6.2f}; ' \\\n",
        "                               'predicted:  {tot_predicted:4d}\\n'.format(tot_prec=tag_results['precision'],\n",
        "                                                                         tot_recall=tag_results['recall'],\n",
        "                                                                         tot_f1=tag_results['f1'],\n",
        "                                                                         tot_predicted=tag_results['n_predicted_entities']))\n",
        "\n",
        "def precision_recall_f1(y_true, y_pred, print_results=True, short_report=False):\n",
        "    # Find all tags\n",
        "    tags = sorted(set(tag[2:] for tag in y_true + y_pred if tag != 'O'))\n",
        "\n",
        "    results = OrderedDict((tag, OrderedDict()) for tag in tags)\n",
        "    n_tokens = len(y_true)\n",
        "    total_correct = 0\n",
        "\n",
        "    # For eval_conll_try we find all chunks in the ground truth and prediction\n",
        "    # For each chunk we store starting and ending indices\n",
        "    for tag in tags:\n",
        "        true_chunk = list()\n",
        "        predicted_chunk = list()\n",
        "        for position in range(n_tokens):\n",
        "            _update_chunk(y_true[position], y_true[position - 1], tag, true_chunk, position)\n",
        "            _update_chunk(y_pred[position], y_pred[position - 1], tag, predicted_chunk, position, True)\n",
        "\n",
        "        _update_last_chunk(true_chunk, position)\n",
        "        _update_last_chunk(predicted_chunk, position)\n",
        "\n",
        "        # Then we find all correctly classified intervals\n",
        "        # True positive results\n",
        "        tp = sum(chunk in predicted_chunk for chunk in true_chunk)\n",
        "        total_correct += tp\n",
        "\n",
        "        # And then just calculate errors of the first and second kind\n",
        "        # False negative\n",
        "        fn = len(true_chunk) - tp\n",
        "        # False positive\n",
        "        fp = len(predicted_chunk) - tp\n",
        "        precision, recall, f1 = _tag_precision_recall_f1(tp, fp, fn)\n",
        "\n",
        "        results[tag]['precision'] = precision\n",
        "        results[tag]['recall'] = recall\n",
        "        results[tag]['f1'] = f1\n",
        "        results[tag]['n_predicted_entities'] = len(predicted_chunk)\n",
        "        results[tag]['n_true_entities'] = len(true_chunk)\n",
        "\n",
        "    total_true_entities, total_predicted_entities, \\\n",
        "           total_precision, total_recall, total_f1, accuracy = _aggregate_metrics(results, total_correct)\n",
        "\n",
        "    if print_results:\n",
        "        #_print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct)\n",
        "        _print_metrics(accuracy, total_precision, total_recall, total_f1)\n",
        "\n",
        "        if not short_report:\n",
        "            for tag, tag_results in results.items():\n",
        "                _print_tag_metrics(tag, tag_results)\n",
        "    return total_f1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR2AFoC0wA3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tags(model, token_idxs_batch):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(token_idxs_batch)\n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            if token_idx != token2idx['<PAD>']:\n",
        "                tags.append(idx2tag[tag_idx])\n",
        "                tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, x_batch)\n",
        "        ground_truth_tags = [idx2tag[tag_idx] for tag_idx in y_batch[0]]\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(tags_batch[0] + ['O'])\n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OEKAjtpOCCz",
        "colab_type": "text"
      },
      "source": [
        "### nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gnPh0ghT1pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IayEDmmwwA3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMModel(torch.nn.Module):\n",
        "    def __init__(self, vocabulary_size, n_tags, PAD_index,\n",
        "                 embedding_dim, rnn_hidden_size,\n",
        "                 dropout_zeroed_probability,\n",
        "                 device='cpu'):\n",
        "        '''\n",
        "        Defines neural network structure.\n",
        "        \n",
        "        architecture: input -> Embedding -> BiLSTM -> Dropout -> Linear\n",
        "        optimizer: Adam\n",
        "        \n",
        "        ----------\n",
        "        Parameters\n",
        "        \n",
        "        vocabulary_size: int, number of words in vocabulary.\n",
        "        n_tags: int, number of tags.\n",
        "        PAD_index: int, index of padding character. Used for loss masking.\n",
        "        embedding_dim: int, dimension of words' embeddings.\n",
        "        rnn_hidden_size: int, number of hidden units in each LSTM cell\n",
        "        dropout_zeroed_probability: float, dropout zeroed probability for Dropout layer.\n",
        "        device: str, cpu or cuda:x\n",
        "        '''\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.embeddng_dim = embedding_dim\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim, padding_idx=PAD_index)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
        "                            hidden_size=rnn_hidden_size,\n",
        "                            num_layers = 2, \n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=dropout_zeroed_probability)\n",
        "        \n",
        "        self.hidden2label = nn.Linear(rnn_hidden_size*2, n_tags)\n",
        "\n",
        "        \n",
        "    def forward(self, x_batch):\n",
        "        '''\n",
        "        Makes forward pass.\n",
        "        \n",
        "        ----------\n",
        "        Parameters\n",
        "        x_batch: torch.LongTensor with shape (number of samples in batch, number words in sentence).\n",
        "        '''\n",
        "        x = self.embedding(x_batch.to('cuda'))\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        y = self.hidden2label(self.dropout(lstm_out))\n",
        "        \n",
        "        return y\n",
        "    \n",
        "    def predict_for_batch(self, x_batch):\n",
        "        '''\n",
        "        Returns predictions for x_batch.\n",
        "        \n",
        "        return type: torch.LongTensor\n",
        "        return shape: (number of samples in batch, number words in sentence.\n",
        "        \n",
        "        ----------\n",
        "        Parameters\n",
        "        x_batch: torch.LongTensor with shape (number of samples in batch, number words in sentence).\n",
        "        '''\n",
        "        y = self.forward(x_batch.to('cuda'))\n",
        "        res = torch.argmax(y, dim=2)\n",
        "        return res\n",
        "    \n",
        "    def train_on_batch(self, x_batch, y_batch, optimizer, loss_function, verbose=0):\n",
        "        '''\n",
        "        Trains model on the given batch.\n",
        "        \n",
        "        ----------\n",
        "        Parameters\n",
        "        x_batch: np.ndarray with shape (number of samples in batch, number words in sentence).\n",
        "        y_batch: np.ndarray with shape (number of samples in batch).\n",
        "        optimizer: torch.optimizer class\n",
        "        loss_function: torch loss class\n",
        "        '''\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        pred = self.forward(x_batch.to('cuda'))\n",
        "\n",
        "        \n",
        "        loss = 0\n",
        "        for i in range(len(x_batch)):\n",
        "          pr = pred[i]\n",
        "          tr = y_batch[i].to('cuda')\n",
        "          mask = x_batch[i] != 1\n",
        "          loss += loss_function(pr,tr)\n",
        "        \n",
        "        if verbose:\n",
        "          print('Loss : %.3f' % loss)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), .6)\n",
        "        optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7iCF1zSwA3w",
        "colab_type": "text"
      },
      "source": [
        "### Эксперименты\n",
        "\n",
        "Задайте BiLSTMModel. Рекомендуемые параметры:\n",
        "- *batch_size*: 32;\n",
        "- начальное значение *learning_rate*: 0.01-0.001\n",
        "- *dropout_zeroed_probability*: 0.7-0.9\n",
        "- *embedding_dim*: 100-200\n",
        "- *rnn_hidden_size*: 150-200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC5ERJpowA3w",
        "colab_type": "text"
      },
      "source": [
        "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1 мере на валидационной и тестовой выборках было не меньше 0.35. \n",
        "\n",
        "Если сеть плохо обучается, попробуйте использовать следующие модификации:\n",
        "    * используйте gradient clipping\n",
        "    * на каждой итерации уменьшайте learning rate (например, в 1.1 раз)\n",
        "    * попробуйте вместо Adam другие оптимизаторы \n",
        "    * экспериментируйте с dropout\n",
        "\n",
        "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuHNuJtc1aZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmcEKXk-wA3x",
        "colab_type": "code",
        "outputId": "6e89fc7c-6d73-46a4-9407-d194ed40c1b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = BiLSTMModel(vocabulary_size = len(token2idx),\n",
        "                    n_tags = len(tag2idx),\n",
        "                    PAD_index = 1,\n",
        "                    embedding_dim = 300,#100\n",
        "                    rnn_hidden_size = 300,#180\n",
        "                    dropout_zeroed_probability = 0.84#0.6\n",
        "                    )\n",
        "\n",
        "model.to('cuda')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMModel(\n",
              "  (embedding): Embedding(28821, 300, padding_idx=1)\n",
              "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.84, inplace=False)\n",
              "  (hidden2label): Linear(in_features=600, out_features=23, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jemn762SMlXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_idxs = [[token2idx[el] for el in sent] for sent in train_tokens]\n",
        "train_tag_idxs = [[tag2idx[el] for el in sent] for sent in train_tags]\n",
        "\n",
        "test_idxs = [[token2idx[el] for el in sent] for sent in test_tokens]\n",
        "test_tag_idxs = [[tag2idx[el] for el in sent] for sent in test_tags]\n",
        "\n",
        "eval_idxs = [[token2idx[el] for el in sent] for sent in validation_tokens]\n",
        "eval_tag_idxs = [[tag2idx[el] for el in sent] for sent in validation_tags]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wxsidLY3ukv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e6d4d04-aba6-4769-88d5-f918a6855ab6"
      },
      "source": [
        "%%time\n",
        "lr = 0.005#0.004\n",
        "loss = nn.CrossEntropyLoss()\n",
        "scores = []\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    \n",
        "    print('Epoch %s -------' % epoch)\n",
        "    lr /= 1.1\n",
        "    \n",
        "    batch_gen = batches_generator(\n",
        "        batch_size=32,\n",
        "        tokens_idxs=train_idxs,\n",
        "        tags_idxs=train_tag_idxs,\n",
        "        shuffle=True,\n",
        "        allow_smaller_last_batch=True\n",
        "    )\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    it = 0\n",
        "    for x_batch,y_batch in batch_gen:\n",
        "        it += 1\n",
        "        model.train_on_batch(x_batch,y_batch,optimizer, loss, verbose=(it%(60)==0))\n",
        "\n",
        "    model.eval()\n",
        "    if epoch % 1 == 0:\n",
        "      train_idxs = [[token2idx[el] for el in sent] for sent in train_tokens]\n",
        "      train_tag_idxs = [[tag2idx[el] for el in sent] for sent in train_tags]\n",
        "      tmp = [epoch]\n",
        "      print('eval:', end=' ')\n",
        "      tmp.append(eval_conll(model, eval_idxs, eval_tag_idxs, short_report=True))\n",
        "      print('test:', end=' ')\n",
        "      tmp.append(eval_conll(model, test_idxs, test_tag_idxs, short_report=True))\n",
        "      scores.append(tmp)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 -------\n",
            "Loss : 10.359\n",
            "Loss : 8.849\n",
            "Loss : 9.829\n",
            "eval: precision:  45.58%; recall:  12.50%; F1:  19.62\n",
            "\n",
            "test: precision:  44.44%; recall:  11.94%; F1:  18.82\n",
            "\n",
            "Epoch 1 -------\n",
            "Loss : 6.213\n",
            "Loss : 7.369\n",
            "Loss : 5.957\n",
            "eval: precision:  39.03%; recall:  25.56%; F1:  30.89\n",
            "\n",
            "test: precision:  40.53%; recall:  25.21%; F1:  31.08\n",
            "\n",
            "Epoch 2 -------\n",
            "Loss : 1.548\n",
            "Loss : 3.305\n",
            "Loss : 2.061\n",
            "eval: precision:  35.79%; recall:  31.72%; F1:  33.63\n",
            "\n",
            "test: precision:  43.31%; recall:  29.52%; F1:  35.11\n",
            "\n",
            "Epoch 3 -------\n",
            "Loss : 2.821\n",
            "Loss : 0.482\n",
            "Loss : 1.116\n",
            "eval: precision:  32.96%; recall:  32.84%; F1:  32.90\n",
            "\n",
            "test: precision:  28.38%; recall:  31.67%; F1:  29.94\n",
            "\n",
            "Epoch 4 -------\n",
            "Loss : 1.134\n",
            "Loss : 1.688\n",
            "Loss : 0.620\n",
            "eval: precision:  32.56%; recall:  33.96%; F1:  33.24\n",
            "\n",
            "test: precision:  38.81%; recall:  33.67%; F1:  36.06\n",
            "\n",
            "Epoch 5 -------\n",
            "Loss : 0.282\n",
            "Loss : 1.384\n",
            "Loss : 1.336\n",
            "eval: precision:  32.64%; recall:  35.07%; F1:  33.81\n",
            "\n",
            "test: precision:  44.21%; recall:  34.83%; F1:  38.96\n",
            "\n",
            "Epoch 6 -------\n",
            "Loss : 0.421\n",
            "Loss : 0.880\n",
            "Loss : 0.180\n",
            "eval: precision:  34.62%; recall:  36.75%; F1:  35.66\n",
            "\n",
            "test: precision:  41.54%; recall:  35.82%; F1:  38.47\n",
            "\n",
            "Epoch 7 -------\n",
            "Loss : 0.130\n",
            "Loss : 0.723\n",
            "Loss : 0.052\n",
            "eval: precision:  35.34%; recall:  36.19%; F1:  35.76\n",
            "\n",
            "test: precision:  43.10%; recall:  33.67%; F1:  37.80\n",
            "\n",
            "Epoch 8 -------\n",
            "Loss : 0.115\n",
            "Loss : 0.913\n",
            "Loss : 0.326\n",
            "eval: precision:  32.87%; recall:  35.63%; F1:  34.20\n",
            "\n",
            "test: precision:  37.32%; recall:  34.66%; F1:  35.94\n",
            "\n",
            "Epoch 9 -------\n",
            "Loss : 0.149\n",
            "Loss : 0.089\n",
            "Loss : 0.688\n",
            "eval: precision:  31.71%; recall:  35.26%; F1:  33.39\n",
            "\n",
            "test: precision:  40.00%; recall:  33.50%; F1:  36.46\n",
            "\n",
            "Epoch 10 -------\n",
            "Loss : 0.221\n",
            "Loss : 1.155\n",
            "Loss : 0.154\n",
            "eval: precision:  31.34%; recall:  35.26%; F1:  33.19\n",
            "\n",
            "test: precision:  42.57%; recall:  34.66%; F1:  38.21\n",
            "\n",
            "Epoch 11 -------\n",
            "Loss : 0.154\n",
            "Loss : 0.083\n",
            "Loss : 0.047\n",
            "eval: precision:  32.43%; recall:  35.82%; F1:  34.04\n",
            "\n",
            "test: precision:  39.78%; recall:  36.82%; F1:  38.24\n",
            "\n",
            "Epoch 12 -------\n",
            "Loss : 0.122\n",
            "Loss : 0.012\n",
            "Loss : 0.025\n",
            "eval: precision:  34.27%; recall:  38.81%; F1:  36.40\n",
            "\n",
            "test: precision:  36.42%; recall:  37.15%; F1:  36.78\n",
            "\n",
            "Epoch 13 -------\n",
            "Loss : 0.005\n",
            "Loss : 0.029\n",
            "Loss : 0.010\n",
            "eval: precision:  32.45%; recall:  38.62%; F1:  35.26\n",
            "\n",
            "test: precision:  40.58%; recall:  37.15%; F1:  38.79\n",
            "\n",
            "Epoch 14 -------\n",
            "Loss : 0.283\n",
            "Loss : 0.825\n",
            "Loss : 0.014\n",
            "eval: precision:  30.84%; recall:  36.94%; F1:  33.62\n",
            "\n",
            "test: precision:  41.19%; recall:  36.82%; F1:  38.88\n",
            "\n",
            "Epoch 15 -------\n",
            "Loss : 0.260\n",
            "Loss : 0.006\n",
            "Loss : 0.328\n",
            "eval: precision:  32.58%; recall:  37.69%; F1:  34.95\n",
            "\n",
            "test: precision:  40.97%; recall:  36.48%; F1:  38.60\n",
            "\n",
            "Epoch 16 -------\n",
            "Loss : 0.006\n",
            "Loss : 0.355\n",
            "Loss : 0.012\n",
            "eval: precision:  33.33%; recall:  38.62%; F1:  35.78\n",
            "\n",
            "test: precision:  40.33%; recall:  36.65%; F1:  38.40\n",
            "\n",
            "Epoch 17 -------\n",
            "Loss : 0.003\n",
            "Loss : 0.022\n",
            "Loss : 0.007\n",
            "eval: precision:  32.41%; recall:  36.94%; F1:  34.52\n",
            "\n",
            "test: precision:  42.94%; recall:  37.31%; F1:  39.93\n",
            "\n",
            "Epoch 18 -------\n",
            "Loss : 0.026\n",
            "Loss : 0.002\n",
            "Loss : 0.279\n",
            "eval: precision:  31.80%; recall:  36.01%; F1:  33.77\n",
            "\n",
            "test: precision:  42.02%; recall:  35.82%; F1:  38.68\n",
            "\n",
            "Epoch 19 -------\n",
            "Loss : 0.021\n",
            "Loss : 0.302\n",
            "Loss : 0.003\n",
            "eval: precision:  32.85%; recall:  37.69%; F1:  35.10\n",
            "\n",
            "test: precision:  39.66%; recall:  35.32%; F1:  37.37\n",
            "\n",
            "Epoch 20 -------\n",
            "Loss : 0.008\n",
            "Loss : 0.005\n",
            "Loss : 0.001\n",
            "eval: precision:  32.06%; recall:  37.69%; F1:  34.65\n",
            "\n",
            "test: precision:  42.36%; recall:  36.32%; F1:  39.11\n",
            "\n",
            "Epoch 21 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.002\n",
            "Loss : 0.000\n",
            "eval: precision:  31.03%; recall:  36.94%; F1:  33.73\n",
            "\n",
            "test: precision:  42.26%; recall:  36.65%; F1:  39.25\n",
            "\n",
            "Epoch 22 -------\n",
            "Loss : 0.052\n",
            "Loss : 0.089\n",
            "Loss : 0.010\n",
            "eval: precision:  32.00%; recall:  37.31%; F1:  34.45\n",
            "\n",
            "test: precision:  40.75%; recall:  36.15%; F1:  38.31\n",
            "\n",
            "Epoch 23 -------\n",
            "Loss : 0.004\n",
            "Loss : 0.169\n",
            "Loss : 0.000\n",
            "eval: precision:  33.39%; recall:  37.13%; F1:  35.16\n",
            "\n",
            "test: precision:  41.89%; recall:  35.99%; F1:  38.72\n",
            "\n",
            "Epoch 24 -------\n",
            "Loss : 0.001\n",
            "Loss : 0.000\n",
            "Loss : 0.034\n",
            "eval: precision:  33.17%; recall:  37.13%; F1:  35.04\n",
            "\n",
            "test: precision:  41.00%; recall:  36.65%; F1:  38.70\n",
            "\n",
            "Epoch 25 -------\n",
            "Loss : 0.001\n",
            "Loss : 0.000\n",
            "Loss : 0.001\n",
            "eval: precision:  32.42%; recall:  37.50%; F1:  34.78\n",
            "\n",
            "test: precision:  41.27%; recall:  35.66%; F1:  38.26\n",
            "\n",
            "Epoch 26 -------\n",
            "Loss : 0.016\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  32.20%; recall:  37.13%; F1:  34.49\n",
            "\n",
            "test: precision:  41.19%; recall:  35.66%; F1:  38.22\n",
            "\n",
            "Epoch 27 -------\n",
            "Loss : 0.268\n",
            "Loss : 0.000\n",
            "Loss : 0.006\n",
            "eval: precision:  32.16%; recall:  37.50%; F1:  34.63\n",
            "\n",
            "test: precision:  40.44%; recall:  36.48%; F1:  38.36\n",
            "\n",
            "Epoch 28 -------\n",
            "Loss : 0.003\n",
            "Loss : 0.000\n",
            "Loss : 0.058\n",
            "eval: precision:  32.63%; recall:  37.50%; F1:  34.90\n",
            "\n",
            "test: precision:  41.07%; recall:  36.98%; F1:  38.92\n",
            "\n",
            "Epoch 29 -------\n",
            "Loss : 0.001\n",
            "Loss : 0.196\n",
            "Loss : 0.003\n",
            "eval: precision:  32.52%; recall:  37.50%; F1:  34.84\n",
            "\n",
            "test: precision:  39.67%; recall:  36.32%; F1:  37.92\n",
            "\n",
            "Epoch 30 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  32.80%; recall:  38.06%; F1:  35.23\n",
            "\n",
            "test: precision:  39.46%; recall:  36.32%; F1:  37.82\n",
            "\n",
            "Epoch 31 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.001\n",
            "Loss : 0.002\n",
            "eval: precision:  32.28%; recall:  38.06%; F1:  34.93\n",
            "\n",
            "test: precision:  39.43%; recall:  36.48%; F1:  37.90\n",
            "\n",
            "Epoch 32 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.002\n",
            "eval: precision:  32.27%; recall:  37.87%; F1:  34.85\n",
            "\n",
            "test: precision:  39.15%; recall:  36.48%; F1:  37.77\n",
            "\n",
            "Epoch 33 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.078\n",
            "Loss : 0.001\n",
            "eval: precision:  32.22%; recall:  37.69%; F1:  34.74\n",
            "\n",
            "test: precision:  39.78%; recall:  36.82%; F1:  38.24\n",
            "\n",
            "Epoch 34 -------\n",
            "Loss : 0.004\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  32.85%; recall:  37.69%; F1:  35.10\n",
            "\n",
            "test: precision:  40.18%; recall:  36.32%; F1:  38.15\n",
            "\n",
            "Epoch 35 -------\n",
            "Loss : 0.002\n",
            "Loss : 0.000\n",
            "Loss : 0.002\n",
            "eval: precision:  32.95%; recall:  37.87%; F1:  35.24\n",
            "\n",
            "test: precision:  39.89%; recall:  36.32%; F1:  38.02\n",
            "\n",
            "Epoch 36 -------\n",
            "Loss : 0.003\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.17%; recall:  37.87%; F1:  35.37\n",
            "\n",
            "test: precision:  39.78%; recall:  36.15%; F1:  37.88\n",
            "\n",
            "Epoch 37 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.020\n",
            "Loss : 0.000\n",
            "eval: precision:  32.74%; recall:  37.69%; F1:  35.04\n",
            "\n",
            "test: precision:  39.63%; recall:  35.82%; F1:  37.63\n",
            "\n",
            "Epoch 38 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  32.48%; recall:  37.69%; F1:  34.89\n",
            "\n",
            "test: precision:  39.82%; recall:  36.32%; F1:  37.99\n",
            "\n",
            "Epoch 39 -------\n",
            "Loss : 0.001\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  32.63%; recall:  37.69%; F1:  34.98\n",
            "\n",
            "test: precision:  40.29%; recall:  36.48%; F1:  38.29\n",
            "\n",
            "Epoch 40 -------\n",
            "Loss : 0.012\n",
            "Loss : 0.231\n",
            "Loss : 0.055\n",
            "eval: precision:  32.90%; recall:  37.50%; F1:  35.05\n",
            "\n",
            "test: precision:  40.41%; recall:  35.99%; F1:  38.07\n",
            "\n",
            "Epoch 41 -------\n",
            "Loss : 0.001\n",
            "Loss : 0.133\n",
            "Loss : 0.000\n",
            "eval: precision:  33.22%; recall:  37.69%; F1:  35.31\n",
            "\n",
            "test: precision:  40.45%; recall:  36.15%; F1:  38.18\n",
            "\n",
            "Epoch 42 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.22%; recall:  37.69%; F1:  35.31\n",
            "\n",
            "test: precision:  40.67%; recall:  36.15%; F1:  38.28\n",
            "\n",
            "Epoch 43 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.002\n",
            "eval: precision:  33.33%; recall:  37.69%; F1:  35.38\n",
            "\n",
            "test: precision:  40.56%; recall:  35.99%; F1:  38.14\n",
            "\n",
            "Epoch 44 -------\n",
            "Loss : 0.004\n",
            "Loss : 0.003\n",
            "Loss : 0.000\n",
            "eval: precision:  33.22%; recall:  37.31%; F1:  35.15\n",
            "\n",
            "test: precision:  41.17%; recall:  36.32%; F1:  38.59\n",
            "\n",
            "Epoch 45 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.004\n",
            "eval: precision:  33.22%; recall:  37.13%; F1:  35.07\n",
            "\n",
            "test: precision:  40.67%; recall:  36.15%; F1:  38.28\n",
            "\n",
            "Epoch 46 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.001\n",
            "eval: precision:  33.61%; recall:  37.87%; F1:  35.61\n",
            "\n",
            "test: precision:  40.45%; recall:  36.15%; F1:  38.18\n",
            "\n",
            "Epoch 47 -------\n",
            "Loss : 0.059\n",
            "Loss : 0.000\n",
            "Loss : 0.001\n",
            "eval: precision:  33.67%; recall:  37.87%; F1:  35.65\n",
            "\n",
            "test: precision:  40.37%; recall:  36.15%; F1:  38.15\n",
            "\n",
            "Epoch 48 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.55%; recall:  37.69%; F1:  35.50\n",
            "\n",
            "test: precision:  40.63%; recall:  36.32%; F1:  38.35\n",
            "\n",
            "Epoch 49 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.035\n",
            "eval: precision:  33.55%; recall:  37.69%; F1:  35.50\n",
            "\n",
            "test: precision:  40.71%; recall:  36.32%; F1:  38.39\n",
            "\n",
            "Epoch 50 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.56%; recall:  37.50%; F1:  35.42\n",
            "\n",
            "test: precision:  40.75%; recall:  36.15%; F1:  38.31\n",
            "\n",
            "Epoch 51 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.053\n",
            "Loss : 0.058\n",
            "eval: precision:  33.56%; recall:  37.31%; F1:  35.34\n",
            "\n",
            "test: precision:  40.82%; recall:  36.15%; F1:  38.35\n",
            "\n",
            "Epoch 52 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.001\n",
            "eval: precision:  33.39%; recall:  36.94%; F1:  35.08\n",
            "\n",
            "test: precision:  40.82%; recall:  36.15%; F1:  38.35\n",
            "\n",
            "Epoch 53 -------\n",
            "Loss : 0.443\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.56%; recall:  37.31%; F1:  35.34\n",
            "\n",
            "test: precision:  40.90%; recall:  36.15%; F1:  38.38\n",
            "\n",
            "Epoch 54 -------\n",
            "Loss : 0.008\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.56%; recall:  37.31%; F1:  35.34\n",
            "\n",
            "test: precision:  40.52%; recall:  36.15%; F1:  38.21\n",
            "\n",
            "Epoch 55 -------\n",
            "Loss : 0.019\n",
            "Loss : 0.067\n",
            "Loss : 0.002\n",
            "eval: precision:  33.61%; recall:  37.50%; F1:  35.45\n",
            "\n",
            "test: precision:  40.48%; recall:  36.32%; F1:  38.29\n",
            "\n",
            "Epoch 56 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.204\n",
            "eval: precision:  33.61%; recall:  37.69%; F1:  35.53\n",
            "\n",
            "test: precision:  40.41%; recall:  36.32%; F1:  38.25\n",
            "\n",
            "Epoch 57 -------\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "Loss : 0.000\n",
            "eval: precision:  33.50%; recall:  37.69%; F1:  35.47\n",
            "\n",
            "test: precision:  40.45%; recall:  36.15%; F1:  38.18\n",
            "\n",
            "Epoch 58 -------\n",
            "Loss : 0.002\n",
            "Loss : 0.000\n",
            "Loss : 0.056\n",
            "eval: precision:  33.50%; recall:  37.69%; F1:  35.47\n",
            "\n",
            "test: precision:  40.30%; recall:  36.15%; F1:  38.11\n",
            "\n",
            "Epoch 59 -------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ea262b978656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr = 0.005#0.004\\nloss = nn.CrossEntropyLoss()\\nscores = []\\nfor epoch in range(100):\\n    model.train()\\n    \\n    print('Epoch %s -------' % epoch)\\n    lr /= 1.1\\n    \\n    batch_gen = batches_generator(\\n        batch_size=32,\\n        tokens_idxs=train_idxs,\\n        tags_idxs=train_tag_idxs,\\n        shuffle=True,\\n        allow_smaller_last_batch=True\\n    )\\n    \\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n    \\n    it = 0\\n    for x_batch,y_batch in batch_gen:\\n        it += 1\\n        model.train_on_batch(x_batch,y_batch,optimizer, loss, verbose=(it%(60)==0))\\n\\n    model.eval()\\n    if epoch % 1 == 0:\\n      train_idxs = [[token2idx[el] for el in sent] for sent in train_tokens]\\n      train_tag_idxs = [[tag2idx[el] for el in sent] for sent in train_tags]\\n      tmp = [epoch]\\n      print('eval:', end=' ')\\n      tmp.append(eval_conll(model, eval_idxs, eval_tag_idxs, short_report=True))\\n      print('test:', end=' ')\\n      tmp.append(eval_conll(model, test_idxs, test_tag_idxs, short_report=True))\\n      scores.append(tmp)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-16b60a567efd>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x_batch, y_batch, optimizer, loss_function, verbose)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss : %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V56AbO_UwA32",
        "colab_type": "text"
      },
      "source": [
        "## Бонусная часть. Улучшение качества теггера (4 балла).\n",
        "\n",
        "Улучшите качество теггера на данной задаче.\n",
        "\n",
        "Бонусные баллы будут начисляться в зависимости от результата f1-меры (одновременно на тестовой и валидационной выборках!).\n",
        "\n",
        "+ 1 балл — $> 0.38$\n",
        "+ 2 балла — $> 0.4$\n",
        "+ 3 балла — $> 0.425$\n",
        "+ 4 балла — $> 0.45$\n",
        "\n",
        "Разрешается использовать любые разумные способы (в том числе и не рассматривающиеся в курсе). Под неразумными способами понимаются любые, в которых используются модели, обученные на dev или test, а также модели, использующие утечки в данных, не относяющиеся к смыслу задачи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t89VY9sEwA32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################\n",
        "######### YOUR CODE HERE #############\n",
        "######################################        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}