{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "lab_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ON3NC6gDXicN",
        "UEEiZByVXidG"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50SEl2pbXib0",
        "colab_type": "text"
      },
      "source": [
        "# Практическое задание 3 \n",
        "\n",
        "# Классификация предложений с использованием BERT\n",
        "\n",
        "## курс \"Математические методы анализа текстов\"\n",
        "\n",
        "\n",
        "### ФИО: Хуршудов Артем Эрнестович"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Obg2A2ZXib2",
        "colab_type": "text"
      },
      "source": [
        "## Введение\n",
        "\n",
        "### Постановка задачи\n",
        "\n",
        "В этом задании вы будете классифицировать предложения из медицинских статей на несколько классов (background, objective и т.д.). \n",
        "Для того, чтобы улучшить качество решения вам предлагается дообучить предобученную нейросетевую архитектуру BERT.\n",
        "\n",
        "### Библиотеки\n",
        "\n",
        "Для этого задания вам понадобятся следующие библиотеки:\n",
        " - [Pytorch](https://pytorch.org/).\n",
        " - [Transformers](https://github.com/huggingface/transformers).\n",
        " \n",
        "### Данные\n",
        "\n",
        "Скачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/13HlWH8jnmsxqDKrEptxOXQg9kkuQMmGq/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN8NzhHbXib4",
        "colab_type": "text"
      },
      "source": [
        "## Часть 1. Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTRyyXwVXib5",
        "colab_type": "text"
      },
      "source": [
        "Мы будем работать с предложениями из медицинских статей, разбитых на несколько классов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOvVcxcfXib7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAHgGRiGXib-",
        "colab_type": "text"
      },
      "source": [
        "Путь к папке с данными:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy7VXWNTXicA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = \"./\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFNJ7RXbXicD",
        "colab_type": "text"
      },
      "source": [
        "Функция считывания данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZnHpbTQXicF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(file_name):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name : str\n",
        "        Pubmed sentences file path\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    text_data : list of str\n",
        "        List of sentences for algorithm\n",
        "    \n",
        "    target_data : list of str\n",
        "        List of sentence categories\n",
        "    \"\"\"\n",
        "    text_data = []\n",
        "    target_data = []\n",
        "\n",
        "    with open(file_name, 'r') as f_input:\n",
        "        for line in f_input:\n",
        "            if line.startswith('#') or line == '\\n':\n",
        "                continue\n",
        "            target, text = line.split('\\t')[:2]    \n",
        "\n",
        "            text_data.append(text)\n",
        "            target_data.append(target)\n",
        "    \n",
        "    return text_data, target_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gui9mJFkXicI",
        "colab_type": "text"
      },
      "source": [
        "Считывание данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzBVC_NgXicJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, train_target = read_data(f'{DATA_PATH}/data_train.txt')\n",
        "test_data, test_target = read_data(f'{DATA_PATH}/data_test.txt')\n",
        "dev_data, dev_target = read_data(f'{DATA_PATH}/data_dev.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON3NC6gDXicN",
        "colab_type": "text"
      },
      "source": [
        "## Часть 2. Построение бейзлайна (1 балл)\n",
        "\n",
        "В этой части задания вам необходимо построить бейзлайн модель, с которой вы будете сравнивать ваше решение. В качестве бейзлайна вам предлагается использовать модель логистической регрессии на tf-idf представлениях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ruNkR1CXicO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53dxd5xpXicR",
        "colab_type": "text"
      },
      "source": [
        "Перед тем как подать в модель предложения, необходимо их предобработать:\n",
        "    \n",
        "1. привести все предложения к нижнему регистру\n",
        "2. удалить из предложений все непробельные символы кроме букв, цифр\n",
        "3. все цифры заменить на нули\n",
        "\n",
        "Метки ответов необходимо преобразовать из текстового вида в числовой (это можно сделать с помощью LabelEncoder).\n",
        "\n",
        "Затем необходимо построить tf-idf матрицу по выбранным предложениям (используйте для подсчёта tf-idf только train_data!) и обучить на них модель логистической регрессии. Используйте dev выборку для подбора гиперпараметров модели. Добейтесь того, что на test и dev выборках accuracy будет будет выше 0.8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFD5lp9qXicS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "train_target = le.fit_transform(train_target)\n",
        "test_target = le.transform(test_target)\n",
        "dev_target = le.transform(dev_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9zWYpS9XicT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "regex1 = re.compile('[^a-z0-9\\s]')\n",
        "regex2 = re.compile('[0-9]')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = regex1.sub('', text)\n",
        "    text = regex2.sub('0', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O7kQNwWXicV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_prep = [preprocess(el) for el in train_data]\n",
        "test_data_prep = [preprocess(el) for el in test_data]\n",
        "dev_data_prep = [preprocess(el) for el in dev_data]\n",
        "\n",
        "# train_data = [preprocess(el) for el in train_data]\n",
        "# test_data = [preprocess(el) for el in test_data]\n",
        "# dev_data = [preprocess(el) for el in dev_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O56vCSOoXica",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf = TfidfVectorizer(ngram_range=(1,2))\n",
        "train_data_tf = tf.fit_transform(train_data_prep)\n",
        "test_data_tf = tf.transform(test_data_prep)\n",
        "dev_data_tf = tf.transform(dev_data_prep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHbsodNNXicc",
        "colab_type": "code",
        "outputId": "ea85c638-9b36-4d53-b726-515e058e3645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "clf = LogisticRegression(multi_class='auto', C=3)\n",
        "clf.fit(train_data_tf, train_target)\n",
        "\n",
        "print(accuracy_score(clf.predict(train_data_tf), train_target))\n",
        "print(accuracy_score(clf.predict(dev_data_tf), dev_target))\n",
        "print(accuracy_score(clf.predict(test_data_tf), test_target))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9568711219611433\n",
            "0.8058205447255634\n",
            "0.8034158978747348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJZzSbZXice",
        "colab_type": "text"
      },
      "source": [
        "## Часть 3. Задание BERT (4 балла за 3 и 4 части)\n",
        "\n",
        "Так как обучающих предложений очень мало, попробуем использовать модель BERT, предобученную на большом датасете. Будем использовать библиотеку transformers. Для обучения модели используйте данные до обработки из предыдущего пункта."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XbFwfiea1tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2lCZ2YWXicf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
        "NUM_LABELS = len(set(train_target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzdSj1E0Xicg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1aB5DeXich",
        "colab_type": "text"
      },
      "source": [
        "Модель BERT работает с специальным форматом данных — все токены из предложения получены с помощью алгоритма BPE. Класс BertTokenizer позволяет получить BPE разбиение для предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77TczBlkXich",
        "colab_type": "code",
        "outputId": "c099aa85-ae89-4c66-f222-e48df8e90f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 426041.83B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o288hpcDXicj",
        "colab_type": "text"
      },
      "source": [
        "В библиотеке transformers есть специальный класс для работы с задачей классификации — BertForSequenceClassification. Воспользуемся им, чтобы задать модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "C0loA-MxXick",
        "colab_type": "code",
        "outputId": "107b6a3f-6343-42f2-b650-747437fc738c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    BERT_MODEL_NAME, num_labels=NUM_LABELS\n",
        ")\n",
        "\n",
        "bert_model.to('cuda')\n",
        "##"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 138424.42B/s]\n",
            "100%|██████████| 440473133/440473133 [00:36<00:00, 12073676.08B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkaRjTgcXicl",
        "colab_type": "text"
      },
      "source": [
        "Реализуем специальный кастомный датасет для токенизированных с помощью BPE предложений. Каждое предложение должно быть преобразовано в последовательность BPE индексов. Не забудьте, что в начале каждого предложения должен стоять специальный токен [CLS], а в конце должен стоять специальный токен [SEP].\n",
        "\n",
        "Задайте датасет, используя BertTokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5HI_R4jXicm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertTokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenizer, text_data, target_data=None, max_length=256):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokenizer : instance of BertTokenizer\n",
        "        text_data : list of str\n",
        "            List of input sentences\n",
        "        target_data : list of int\n",
        "            List of input targets\n",
        "        max_length : int\n",
        "            Maximum length of input sequence (length in bpe tokens)\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = []\n",
        "        self.target_data = target_data\n",
        "\n",
        "        res_arr = []\n",
        "        for el in text_data:\n",
        "          self.data.append(torch.tensor(tokenizer.encode(el, max_length = max_length, add_special_tokens=True)))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        if self.target_data is not None:\n",
        "            return self.data[i], self.target_data[i]\n",
        "        else:\n",
        "            return self.data[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIcmObQpXicn",
        "colab_type": "text"
      },
      "source": [
        "Получите все датасеты для всех типов данных. \n",
        "\n",
        "**Замечание**. После получения есть смысл сохранить все датасеты на диск, т.к. предобработка занимает время."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr4M2DQNXicn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = BertTokenizedDataset(tokenizer, train_data, train_target)\n",
        "dev_dataset = BertTokenizedDataset(tokenizer, dev_data, dev_target)\n",
        "test_dataset = BertTokenizedDataset(tokenizer, test_data, test_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCbUzS1CXico",
        "colab_type": "text"
      },
      "source": [
        "Используем  класс PadSequences, чтобы задать способ паддинга, работающий с встроенным в pytorch DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iyMzsDLXico",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PadSequences:\n",
        "    def __init__(self, use_labels=False):\n",
        "        self.use_labels = use_labels\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : list of objects or list of (object, label)\n",
        "            Each object is list of int indexes.\n",
        "            Each label is int.\n",
        "        \"\"\"\n",
        "        data_label_batch = batch if self.use_labels else [(x, 0) for x in batch]\n",
        "\n",
        "            \n",
        "        # Sort the batch in the descending order\n",
        "        sorted_batch = sorted(data_label_batch, key=lambda x: x[0].shape[0], reverse=True)\n",
        "        # Get each sequence and pad it\n",
        "        sequences = [x[0] for x in sorted_batch]\n",
        "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
        "        max_lenght = len(sequences[0])\n",
        "\n",
        "        # Also need to store the length of each sequence\n",
        "        # This is later needed in order to unpad the sequences\n",
        "        lengths = torch.LongTensor([[1] * len(x) + [0] * (max_lenght - len(x)) for x in sequences])\n",
        "        # Don't forget to grab the labels of the *sorted* batch\n",
        "        \n",
        "        if self.use_labels:\n",
        "            labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
        "            return sequences_padded, lengths, labels\n",
        "        else:\n",
        "            return sequences_padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq3rhpvQhA2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvAaMFW_Xic7",
        "colab_type": "text"
      },
      "source": [
        "Зададим DataLoader для каждого из датасетов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpSKHp4jXic8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwFBmKDBXic9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=PadSequences(use_labels=True))\n",
        "\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              collate_fn=PadSequences(use_labels=True))\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              collate_fn=PadSequences(use_labels=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTilz9OxXic-",
        "colab_type": "text"
      },
      "source": [
        "Заметьте, что модель трансформера обучается по достаточному большому размеру батча (обычно 64), который скорее всего не будет влезать на вашу видеокарту. Поэтому, рекомендуется \"накапливать\" градиенты за несколько итераций. С помощью параметра ACCUMULATION_STEPS задайте, раз в сколько итераций вам необходимо делать шаг метода оптимизации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_jO1PCVXic-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCH_AMOUNT = 1\n",
        "TRAIN_LENGTH = len(train_dataset)\n",
        "ACCUMULATION_STEPS = 4\n",
        "\n",
        "LR = 2e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th0z2edbXic_",
        "colab_type": "text"
      },
      "source": [
        "Посчитайте общее число раз, когда ваш оптимизатор будет делать обновления на основе выбранных значений EPOCH_AMOUNT, BATCH_SIZE, ACCUMULATION_STEPS и  TRAIN_LENGTH. Эта величина нужна для правильного задания параметров оптимизаторов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5LlPPaqXidA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_optimization_step_amount = TRAIN_LENGTH / BATCH_SIZE / ACCUMULATION_STEPS * EPOCH_AMOUNT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUcciqK6XidA",
        "colab_type": "text"
      },
      "source": [
        "Зададим параметры оптимизаторов. Мы будем использовать специальные оптимизаторы из библиотеки transformers AdamW и WarmupLinearSchedule, обеспечивающие плавный разгон и медленное затухание темпа обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHhkbFyQXidE",
        "colab_type": "text"
      },
      "source": [
        "Для некоторых групп параметров зададим коэффициенты регуляризации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1gEK2w8XidF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(bert_model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(),\n",
        "                     lr=LR)\n",
        "scheduler = WarmupLinearSchedule(\n",
        "    optimizer,\n",
        "    warmup_steps=train_optimization_step_amount * 0.05,\n",
        "    t_total=train_optimization_step_amount,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEEiZByVXidG",
        "colab_type": "text"
      },
      "source": [
        "## Часть 4. Обучение BERT \n",
        "\n",
        "Теперь всё готово к тому, чтобы дообучить BERT на датасете train_dataset!\n",
        "\n",
        "Используйте dev_dataset для выбора гиперпараметров модели и обучения. Задание будет засчтано на полный балл если на dev_dataset и test_dataset точность будет выше 0.84."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W1xyjC9XidG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "device = 'cuda'\n",
        "losses = []\n",
        "\n",
        "for epoch in range(EPOCH_AMOUNT):  \n",
        "  \n",
        "  bert_model.train()  \n",
        "  for batch in train_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    bert_model.train()\n",
        "    outputs = bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    losses.append(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(bert_model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqR-JMfY_7hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flatten = lambda l: [item for sublist in l for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VoF09sbwhky",
        "colab_type": "code",
        "outputId": "91c5b759-050e-4a21-c5a1-0b28deeaeae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## VALIDATION\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "bert_model.eval()\n",
        "\n",
        "pred = []\n",
        "real = []\n",
        "\n",
        "for batch in dev_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  pred_flat = np.argmax(logits, axis=1)\n",
        "  pred.append(pred_flat)\n",
        "  real.append(label_ids)\n",
        "\n",
        "print(accuracy_score(flatten(pred), flatten(real) ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8546246370800498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4BcSmeV_yZ_",
        "colab_type": "code",
        "outputId": "0702b5c0-9ecb-478b-a271-fec51c13d305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## test\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "bert_model.eval()\n",
        "\n",
        "pred = []\n",
        "real = []\n",
        "\n",
        "qqq = len(test_dataloader) / 10\n",
        "\n",
        "for it, batch in enumerate(test_dataloader):\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  pred_flat = np.argmax(logits, axis=1)\n",
        "  pred.append(pred_flat)\n",
        "  real.append(label_ids)\n",
        "\n",
        "print(accuracy_score(flatten(pred), flatten(real)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8509325143305942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgObzADWXidH",
        "colab_type": "text"
      },
      "source": [
        "## Бонусная часть (до 3 баллов)\n",
        "\n",
        "Улучшите качество (на обеих выборках), используя любые способы (кроме использования дополнительных обучающих данных датасета RCT2000):\n",
        "\n",
        "* $> 0.86$ — 1 балл \n",
        "* $> 0.88$ — 2 балла\n",
        "* $> 0.9$ — 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9F_H2xnlo2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = BertTokenizedDataset(tokenizer, train_data, train_target)\n",
        "dev_dataset = BertTokenizedDataset(tokenizer, dev_data, dev_target)\n",
        "test_dataset = BertTokenizedDataset(tokenizer, test_data, test_target)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              collate_fn=PadSequences(use_labels=True))\n",
        "\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              collate_fn=PadSequences(use_labels=True))\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              collate_fn=PadSequences(use_labels=True))\n",
        "\n",
        "EPOCH_AMOUNT = 2\n",
        "TRAIN_LENGTH = len(train_dataset)\n",
        "ACCUMULATION_STEPS = 1\n",
        "\n",
        "LR = 2e-5\n",
        "\n",
        "train_optimization_step_amount = TRAIN_LENGTH / BATCH_SIZE / ACCUMULATION_STEPS * EPOCH_AMOUNT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17FJ28QLoOgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    BERT_MODEL_NAME, num_labels=NUM_LABELS\n",
        ")\n",
        "\n",
        "bert_model.to('cuda')\n",
        "print('ok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwr9c6zKdPVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(bert_model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(#bert_model.parameters(),\n",
        "                  optimizer_grouped_parameters,\n",
        "                  lr=LR, correct_bias=False)\n",
        "scheduler = WarmupLinearSchedule(\n",
        "    optimizer,\n",
        "    warmup_steps=train_optimization_step_amount * 0.05,\n",
        "    t_total=train_optimization_step_amount,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFbdMOvzuxey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def count_acc(model, data_loader, it_max=10000000):\n",
        "  flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  pred = np.array([])\n",
        "  real = np.array([])\n",
        "\n",
        "  for it, batch in enumerate(data_loader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    pred_flat = np.argmax(logits, axis=1)\n",
        "\n",
        "    pred = np.append(pred, [pred_flat])\n",
        "    real = np.append(real, [label_ids])\n",
        "\n",
        "    if it > it_max:\n",
        "      break\n",
        "\n",
        "  return accuracy_score(pred.reshape(-1), real.reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h_p-zadRF7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ad458db-33d1-4a33-d00c-1c6e726e604b"
      },
      "source": [
        "# %%time\n",
        "loss_arr_1 = []\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "for epoch in range(EPOCH_AMOUNT):  \n",
        "  \n",
        "  bert_model.train()  \n",
        "  for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    bert_model.train()\n",
        "    outputs = bert_model(b_input_ids, token_type_ids=None, labels=b_labels)\n",
        "    \n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_arr_1.append(loss)\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "      print(epoch, idx)\n",
        "      c1 = count_acc(bert_model, dev_dataloader, it_max=30)\n",
        "      c2 = count_acc(bert_model, test_dataloader, it_max=30)\n",
        "      print('c1:', c1)\n",
        "      print('c2:', c2)\n",
        "      print(loss)\n",
        "      # if epoch < 1:\n",
        "      #   continue\n",
        "      # if (c1 > 0.86) and (c2 > 0.86):\n",
        "      #   c3 = count_acc(bert_model, dev_dataloader)\n",
        "      #   print('c3:',c3)\n",
        "      #   if c3 > 0.86:\n",
        "      #     c4 = count_acc(bert_model, test_dataloader)\n",
        "      #     print('c4:',c4)\n",
        "      #     if c4 > 0.86:\n",
        "      #       break\n",
        "      print('-' * 100)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0\n",
            "c1: 0.107421875\n",
            "c2: 0.138671875\n",
            "tensor(1.6213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 100\n",
            "c1: 0.775390625\n",
            "c2: 0.767578125\n",
            "tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 200\n",
            "c1: 0.8046875\n",
            "c2: 0.767578125\n",
            "tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 300\n",
            "c1: 0.818359375\n",
            "c2: 0.841796875\n",
            "tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 400\n",
            "c1: 0.849609375\n",
            "c2: 0.857421875\n",
            "tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 500\n",
            "c1: 0.8359375\n",
            "c2: 0.861328125\n",
            "tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 600\n",
            "c1: 0.8515625\n",
            "c2: 0.861328125\n",
            "tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 700\n",
            "c1: 0.8671875\n",
            "c2: 0.869140625\n",
            "tensor(0.8675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 800\n",
            "c1: 0.865234375\n",
            "c2: 0.869140625\n",
            "tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 900\n",
            "c1: 0.869140625\n",
            "c2: 0.880859375\n",
            "tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1000\n",
            "c1: 0.861328125\n",
            "c2: 0.8828125\n",
            "tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1100\n",
            "c1: 0.869140625\n",
            "c2: 0.884765625\n",
            "tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1200\n",
            "c1: 0.873046875\n",
            "c2: 0.8828125\n",
            "tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1300\n",
            "c1: 0.875\n",
            "c2: 0.880859375\n",
            "tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1400\n",
            "c1: 0.875\n",
            "c2: 0.869140625\n",
            "tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1500\n",
            "c1: 0.875\n",
            "c2: 0.87890625\n",
            "tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1600\n",
            "c1: 0.865234375\n",
            "c2: 0.884765625\n",
            "tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1700\n",
            "c1: 0.8671875\n",
            "c2: 0.8828125\n",
            "tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 1800\n",
            "c1: 0.87109375\n",
            "c2: 0.880859375\n",
            "tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 0\n",
            "c1: 0.8671875\n",
            "c2: 0.88671875\n",
            "tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 100\n",
            "c1: 0.869140625\n",
            "c2: 0.89453125\n",
            "tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 200\n",
            "c1: 0.8671875\n",
            "c2: 0.876953125\n",
            "tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 300\n",
            "c1: 0.869140625\n",
            "c2: 0.884765625\n",
            "tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 400\n",
            "c1: 0.869140625\n",
            "c2: 0.880859375\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 500\n",
            "c1: 0.865234375\n",
            "c2: 0.8828125\n",
            "tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 600\n",
            "c1: 0.880859375\n",
            "c2: 0.888671875\n",
            "tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 700\n",
            "c1: 0.875\n",
            "c2: 0.87890625\n",
            "tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 800\n",
            "c1: 0.8828125\n",
            "c2: 0.880859375\n",
            "tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 900\n",
            "c1: 0.8828125\n",
            "c2: 0.88671875\n",
            "tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1000\n",
            "c1: 0.869140625\n",
            "c2: 0.900390625\n",
            "tensor(0.5824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1100\n",
            "c1: 0.8828125\n",
            "c2: 0.892578125\n",
            "tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1200\n",
            "c1: 0.87890625\n",
            "c2: 0.888671875\n",
            "tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1300\n",
            "c1: 0.876953125\n",
            "c2: 0.89453125\n",
            "tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1400\n",
            "c1: 0.880859375\n",
            "c2: 0.892578125\n",
            "tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1500\n",
            "c1: 0.869140625\n",
            "c2: 0.890625\n",
            "tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1600\n",
            "c1: 0.8828125\n",
            "c2: 0.890625\n",
            "tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1700\n",
            "c1: 0.884765625\n",
            "c2: 0.88671875\n",
            "tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1 1800\n",
            "c1: 0.87890625\n",
            "c2: 0.890625\n",
            "tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJYqARYYZbk2",
        "colab_type": "code",
        "outputId": "d3af8d09-10be-4418-889f-524604212c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%time\n",
        "print(count_acc(bert_model, dev_dataloader))\n",
        "print(count_acc(bert_model, test_dataloader))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8657196184155952\n",
            "0.8617170923839779\n",
            "CPU times: user 6min 57s, sys: 2min 46s, total: 9min 44s\n",
            "Wall time: 9min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}